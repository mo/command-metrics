#!/usr/bin/env python

# TODO:
# -- mark single metric as "bulk" (can run in parallel to others) or "perf" (must have machine to itself when it runs)
# -- re-run X times and save average
# -- make target attribute optional
# -- move legend to the right
# -- make color attribute optional (auto generate unique colors from nice scale)
# -- ability to zoom specific date range
# -- minimum_remeasurement_interval per metric
# -- thicker lines
# -- Y value zoom
# -- make little enum for exit codes

import os
import re
import sys
import csv
import sets
import stat
import time
import glob
import json
import errno
import shutil
import string
import hashlib
import platform
import tempfile
import argparse
import datetime
import subprocess

SCRIPT_DIR = os.path.dirname(os.path.realpath(__file__))
SERIES_HOSTNAME_FILENAME_SEPARATOR = "-"

def human_readable_time_span(seconds):
    if seconds < 0:
        return "-" + human_readable_time_span(-1*seconds)

    output = ""
    if seconds == 0 or seconds % 60 != 0:
        output = str(int(seconds % 60)) + "s"

    minutes = seconds // 60
    if minutes % 60 != 0:
        output = str(int(minutes % 60)) + "m" + output

    hours = minutes // 60
    if hours % 24 != 0:
        output = str(int(hours % 24)) + "h" + output

    days = hours // 24
    if days != 0:
        output = str(int(days)) + "d" + output

    return output

def add_to_path_if_exists(target_env, folder):
    if os.path.exists(folder):
        if args.verbose:
            print "[DEBUG] add '%s' to PATH" % folder
        target_env["PATH"] = target_env["PATH"] + ":" + folder

def prepend_to_path_if_exists(target_env, folder):
    if os.path.exists(folder):
        if args.verbose:
            print "[DEBUG] preprend '%s' to PATH" % folder
        target_env["PATH"] = folder + ":" + target_env["PATH"]

def mkdir_p(path):
    try:
        os.makedirs(path)
    except OSError as exc:
        if exc.errno == errno.EEXIST and os.path.isdir(path):
            pass
        else:
            raise

def measure_metric(metric_name, target):
    metric = config["metrics"][metric_name]

    child_env = os.environ.copy()
    # Prepend (rather than append) the metric-scripts to $PATH because we want
    # measurements to have as little dependency on the host machine as possible.
    prepend_to_path_if_exists(child_env, os.path.join(SCRIPT_DIR, "metric-scripts"))
    external_scripts_root = os.path.join(SCRIPT_DIR, "../../external-metric-scripts")
    # If your metrics require scripts for different git repositories you can add
    # each such git repo as a submodule under the "external-metric-scripts" dir
    # and every dir in every such repo will be put on the $PATH during
    # measurement. You can also add metric scripts directly in
    # "external-metric-scripts" without using a git submodule if you want.
    if os.path.isdir(external_scripts_root):
        for dirpath, _, _ in os.walk(external_scripts_root, followlinks=True):
            prepend_to_path_if_exists(child_env, dirpath)
    # Don't prepend this because it has host specific dependencies, it's added
    # only as a convenience.
    add_to_path_if_exists(child_env, "/usr/local/bin")

    command_template = metric["cmd"]
    if args.verbose:
        print "[DEBUG] command template: " + command_template
    try:
        metric_command = command_template % {"target": target}
    except (TypeError, ValueError):
        # TypeError is raised if template string contains %f instead of %%f
        # ValueError is raised if template string contains %!
        print r"[ERROR] Invalid command template for metric '%s'." % metric_name
        print r"[NOTE] % must be escaped as %% and target should be referred to as %(target)s"
        print r"[NOTE] Offending command template was: " + command_template
        sys.exit(1)
    if args.verbose:
        print "[DEBUG] about to run command: " + metric_command

    proc = subprocess.Popen(metric_command, env=child_env, shell=True, stdin=subprocess.PIPE, stdout=subprocess.PIPE)
    stdout_output, stderr_output = proc.communicate()
    stdout_output = stdout_output.strip()
    stderr_output = stderr_output.strip() if stderr_output else None
    if proc.returncode != 0:
        errmsg = ("[WARNING] metric command returned non-zero exit code\n"
                  + "          exitcode: " + str(proc.returncode) + "\n"
                  + "          metric command: " + metric_command + "\n"
                  + "          metric: " + metric_name + "\n"
                  + "          target: " + target + "\n")
        print errmsg
    if args.verbose:
        print "[DEBUG] value (from stdout) was: " + repr(stdout_output)
    try:
        value = float(stdout_output)
    except ValueError:
        # The metric script printed something other than a number to stdout.
        # Possibly an error message or similar, at any rate we have failed to
        # make this particular measurement, suppress error so we can move on to
        # the next scheduled measurement instead.
        errmsg = ("[ERROR] value printed to stdout by metric command was not a number\n"
                  + "       metric: " + metric_name + "\n"
                  + "       metric command: " + metric_command + "\n"
                  + "       target: " + target + "\n"
                  + "       stdout: " + repr(stdout_output) + "\n"
                  + "       stderr: " + repr(stderr_output) + "\n")
        print errmsg
        value = None
    if "cmd_sleep_seconds" in metric:
        if args.force_sleep != None:
            cmd_sleep_seconds = args.force_sleep
        else:
            cmd_sleep_seconds = float(metric["cmd_sleep_seconds"])
        if args.verbose:
            if ("%.3f" % cmd_sleep_seconds).endswith(".000"):
                sleep_time_str = "%.0f" % cmd_sleep_seconds
            else:
                sleep_time_str = "%.3f" % cmd_sleep_seconds
            print "[DEBUG] throttle sleeping for %s seconds" % sleep_time_str
        time.sleep(cmd_sleep_seconds)
    if args.verbose:
        print ""
    return value


class MeasuredData(object):
    CSV_DELIMITER_CHAR = ','
    CSV_QUOTE_CHAR = '"'

    def __init__(self):
        # self.data is a map from series_name to a list of (timestamp_str, value)
        # NOTE: series_name is computed using get_series_name(metric_name, target)
        self.data = {}

    @staticmethod
    def append_data_to_file(timestamp, value, csv_filepath):
        timestamp_str = timestamp.strftime("%Y-%m-%d %H:%M:%S")
        mkdir_p(os.path.dirname(csv_filepath))
        with open(csv_filepath, "ab") as data_file:
            writer = csv.writer(data_file, delimiter=MeasuredData.CSV_DELIMITER_CHAR, quotechar=MeasuredData.CSV_QUOTE_CHAR, quoting=csv.QUOTE_MINIMAL)
            writer.writerow([timestamp_str, value])

    def add_data_from_file(self, datafile):
        basename = os.path.basename(datafile)
        series_name, _ = basename.split(SERIES_HOSTNAME_FILENAME_SEPARATOR, 1)
        with open(datafile, 'rb') as csvfile:
            csv_reader = csv.reader(csvfile, delimiter=MeasuredData.CSV_DELIMITER_CHAR, quotechar=MeasuredData.CSV_QUOTE_CHAR)
            for row in csv_reader:
                try:
                    [timestamp_str, value] = row
                    self.data.setdefault(series_name, []).append((timestamp_str, value))
                except ValueError as e:
                    print "ERROR: invalid line in incoming data file: " + datafile
                    print "Details: " + repr(e)
                    sys.exit(1)

    def sort(self):
        for _, series_data in self.data.iteritems():
            series_data.sort()

    @staticmethod
    def load_from_dir(data_dir):
        measured_data = MeasuredData()
        for fn in os.listdir(data_dir):
            csv_filepath = os.path.join(data_dir, fn)
            measured_data.add_data_from_file(csv_filepath)
        measured_data.sort()
        return measured_data

    @staticmethod
    def load_from_file(csv_filepath):
        measured_data = MeasuredData()
        measured_data.add_data_from_file(csv_filepath)
        return measured_data

    def save_to_file(self, csv_filepath):
        mkdir_p(os.path.dirname(csv_filepath))
        with open(csv_filepath, "wb") as data_file:
            writer = csv.writer(data_file, delimiter=MeasuredData.CSV_DELIMITER_CHAR, quotechar=MeasuredData.CSV_QUOTE_CHAR, quoting=csv.QUOTE_MINIMAL)
            for series_name, series_data in self.data.iteritems():
                for [timestamp_str, value] in series_data:
                    writer.writerow([series_name, timestamp_str, value])

    def get_series_data(self, series_name):
        return self.data.get(series_name, [])

    # Returns True is something was deleted.
    def delete_series(self, series_name):
        return self.data.pop(series_name, None) != None


def measure_series(selected_series):
    day_dirs_written_to = set()
    total = len(selected_series)
    if args.verbose:
        print "[DEBUG] prepared todo list of %d things to measure" % total

    for (idx, (metric_name, target)) in enumerate(selected_series):
        if not args.quiet:
            print_dict = {
                "current": idx+1,
                "total": total,
                "metric_name": metric_name,
                "target": target,
            }
            if target:
                print "[%(current)d/%(total)d] measuring '%(metric_name)s' for '%(target)s'" % print_dict
            else:
                print "[%(current)d/%(total)d] measuring '%(metric_name)s'" % print_dict
        value = measure_metric(metric_name, target)

        # value == None means that the measurement failed (e.g. a metric script
        # printed non-number to stdout), start the next measurement instead.
        if value != None:
            now = datetime.datetime.now()
            (year, month, day) = get_year_month_day(now)
            data_dir_today = os.path.join(data_root_dir, year, month, day)
            series_name = get_series_name(metric_name, target)
            csv_filepath = os.path.join(data_dir_today, series_name + SERIES_HOSTNAME_FILENAME_SEPARATOR + platform.node() + ".csv")
            MeasuredData.append_data_to_file(now, value, csv_filepath)
            day_dirs_written_to |= {(year, month, day)}

    return day_dirs_written_to

def get_year_month_day(some_datetime):
    return (some_datetime.strftime("%Y"), some_datetime.strftime("%m"), some_datetime.strftime("%d"))

def subcommand_measure():
    if args.metric:
        for metric_name in args.metric:
            if not any(serie["metric_name"] == metric_name for serie in config["unique_series"]):
                print "ERROR: invalid metric name specified in --metric/-m parameter: " + metric_name
                sys.exit(1)

    if args.chart:
        for chart_id in args.chart:
            if not any(chart["chart_id"] == chart_id for chart in config["charts"]):
                print "ERROR: invalid chart_id specified in --chart/-c parameter: " + chart_id
                sys.exit(1)

    # Use unbuffered stdout
    sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 0)

    # If "--metric foo --metric yada" is passed, then only re-take measurements
    # for those metrics. If --chart foo --chart bar is passed, then re-take
    # measurements for all series (metric/target pairs) that appears in those
    # charts. If --metric and --chart parameters are passed simultaneously then
    # re-take measurements only for series in the passed chart, that is also
    # using the specified metric.
    selected_series = set()
    for chart in config["charts"]:
        for serie in chart["chart_series"]:

            metric_name = serie["metric"]
            target = serie.get("target", "")

            if args.metric and not metric_name in args.metric:
                continue

            if args.chart and not chart["chart_id"] in args.chart:
                continue

            selected_series.add((metric_name, target))


    measurement_start_time = datetime.datetime.now()
    if not args.quiet:
        print "Measurement started: " + str(measurement_start_time)[:19]

    day_dirs_written_to = measure_series(selected_series)

    measurement_stoptime = datetime.datetime.now()
    measurement_seconds = (measurement_stoptime - measurement_start_time).total_seconds()
    if not args.quiet:
        print "Measurement stopped: " + str(measurement_stoptime)[:19]
        print "Measurement took: " + human_readable_time_span(measurement_seconds)

    # day_dirs_written_to will typically be an array of a single day, except for
    # example if measure_series() started at 23:50 and took 20 mins to run.
    update_jscache(day_dirs_written_to)

def subcommand_ls():
    series_name_len = max([len(serie["name"]) for serie in config["unique_series"]])
    metric_name_len = max([len(serie["metric_name"]) for serie in config["unique_series"]])
    target_len = max([len(serie.get("target", "")) for serie in config["unique_series"]])

    if not args.quiet:
        print "%s  %s  %s" % ("METRIC".ljust(metric_name_len),
                              "TARGET".ljust(target_len),
                              "SERIES".ljust(series_name_len))
    for serie in config["unique_series"]:
        series_name = serie["name"]
        metric_name = serie["metric_name"]
        target = serie.get("target", "")
        print "%s  %s  %s" % (metric_name.ljust(metric_name_len),
                              target.ljust(target_len),
                              series_name.ljust(series_name_len))

def dir_last_modified(target_dir):
    files = glob.glob(os.path.join(target_dir, "*"))
    return max([os.path.getmtime(f) for f in files])

def rebuild_jscache_day(year, month, day):
    day_jscache_dir = os.path.join(jscache_root_dir, year, month, day)
    if os.path.exists(day_jscache_dir):
        shutil.rmtree(day_jscache_dir)

    day_data_dir = os.path.join(data_root_dir, year, month, day)
    if not os.path.exists(day_data_dir):
        if not args.quiet:
            print "Deleted stale jscache for %s-%s-%s" % (year, month, day)
        # All data for this day has been deleted, deleting the corresponding
        # jscache dir (which we just did above) is the only thing we need to do.
        return

    if not args.quiet:
        print "Rebuilding jscache for %s-%s-%s" % (year, month, day)

    measured_data = MeasuredData.load_from_dir(day_data_dir)
    mkdir_p(day_jscache_dir)

    for serie in config["unique_series"]:
        series_name = serie["name"]
        series_data = measured_data.get_series_data(series_name)
        if series_data:
            serie_jscache_file = os.path.join(day_jscache_dir, series_name)
            jsvals = []
            for timestamp_str, value in series_data:
                timestamp = datetime.datetime.strptime(timestamp_str, "%Y-%m-%d %H:%M:%S")
                timestamp_unix = timestamp.strftime("%s")
                jsvals.append("{x:%s,y:%s}," % (timestamp_unix, value))
            all_jsvals = "".join(jsvals)
            with open(serie_jscache_file + ".jsv", "w") as fil:
                fil.write(all_jsvals)

def reconcatenate_jscache_dir_from_subdirs(jscache_dir):
    for existing_jsv in glob.glob(jscache_dir + "/*.jsv"):
        os.remove(existing_jsv)
    for serie in config["unique_series"]:
        series_name = serie["name"]
        jscache_concatenated_jsv = os.path.join(jscache_dir, series_name + ".jsv")
        subdir_jsv_files = sorted(glob.glob(jscache_dir + "/*/" + series_name + ".jsv"))
        with open(jscache_concatenated_jsv, "wb") as concatenated:
            for subdir_jsv in subdir_jsv_files:
                shutil.copyfileobj(open(subdir_jsv, 'rb'), concatenated)

def rebuild_toplevel_jsdata():
    jsdata_tempfile_oshandle, jsdata_tempfile_name = tempfile.mkstemp()
    with open(jsdata_tempfile_name, "w") as jsdata_temp_file:
        for serie in config["unique_series"]:
            series_name = serie["name"]
            jsdata_temp_file.write("const %s = [\n" % series_name)
            series_jsv = os.path.join(jscache_root_dir, series_name + ".jsv")
            shutil.copyfileobj(open(series_jsv, 'rb'), jsdata_temp_file)
            jsdata_temp_file.write("\n];\n\n")

        jsdata_temp_file.write("allChartData = [\n")
        for chart in config["charts"]:
            jsdata_temp_file.write("  {\n")
            jsdata_temp_file.write("    'chartKey': '%s',\n" % chart["chart_id"])
            jsdata_temp_file.write("    'chartTitle': %s,\n" % json.dumps(chart["chart_title"]))
            jsdata_temp_file.write("    'chartSeries': [\n")

            for chart_series in chart["chart_series"]:
                metric_name = chart_series["metric"]
                target = chart_series.get("target", None)
                metric = config["metrics"][metric_name]
                series_title = chart_series.get("title", None)
                if not series_title:
                    series_title = metric.get("title", metric["cmd"]) % {"target": target}
                jsdata_temp_file.write("            {\n")
                # using json.dumps() to escape string in case it contains ' and " etc
                jsdata_temp_file.write("              'name': %s,\n" % json.dumps(series_title))
                jsdata_temp_file.write("              'color': '%s',\n" % chart_series["color"])
                jsdata_temp_file.write("              'data': %s\n" % get_series_name(metric_name, target))
                jsdata_temp_file.write("            },\n")
            jsdata_temp_file.write("    ],\n")

            if "annotations" in chart:
                jsdata_temp_file.write("    chartAnnotations: {\n")
                for annotation_set_name in chart["annotations"]:
                    annotations = config["annotation_sets"][annotation_set_name]
                    for date, description in annotations.iteritems():
                        timestamp = datetime.datetime.strptime(date, "%Y-%m-%d").strftime("%s")
                        jsdata_temp_file.write("            '%s': %s,\n" % (timestamp, json.dumps(description)))
                jsdata_temp_file.write("    }\n")

            jsdata_temp_file.write("  },\n")
        jsdata_temp_file.write("]\n")

    os.close(jsdata_tempfile_oshandle)
    shutil.move(jsdata_tempfile_name, jsdata_filename)
    os.chmod(jsdata_filename, stat.S_IRUSR | stat.S_IWUSR | stat.S_IROTH)

def update_jscache(days_to_update):
    for (year, month, day) in days_to_update:
        rebuild_jscache_day(year, month, day)

    months_to_update = set([(year, month) for (year, month, day) in days_to_update])
    for (year, month) in months_to_update:
        jsv_month_dir = os.path.join(jscache_root_dir, year, month)
        reconcatenate_jscache_dir_from_subdirs(jsv_month_dir)

    years_to_update = set([year for (year, month, day) in days_to_update])
    for year in years_to_update:
        jsv_year_dir = os.path.join(jscache_root_dir, year)
        reconcatenate_jscache_dir_from_subdirs(jsv_year_dir)

    reconcatenate_jscache_dir_from_subdirs(jscache_root_dir)

    rebuild_toplevel_jsdata()

def subcommand_jscache():
    # Figure out which days that needs to have their jscache entries rebuilt.
    days_to_update = set()
    for day_dir in glob.glob(os.path.join(data_root_dir, "*/*/*")):
        m = re.search("/([0-9]{4})/([0-9]{2})/([0-9]{2})$", day_dir)
        if not m:
            print "WARNING: junk in data dir at: " + day_dir
            continue
        year, month, day = m.groups()
        jscache_day_dir = os.path.join(jscache_root_dir, year, month, day)
        if not os.path.isdir(jscache_day_dir) or os.listdir(jscache_day_dir) == []:
            days_to_update |= {(year, month, day)}
        else:
            data_last_modified = dir_last_modified(day_dir)
            jscache_last_modified = dir_last_modified(jscache_day_dir)
            if data_last_modified > jscache_last_modified:
                days_to_update |= {(year, month, day)}

    # Find days we have jscache data for but where the csv data is deleted.
    for jscache_day_dir in glob.glob(os.path.join(jscache_root_dir, "*/*/*/.")):
        m = re.search(r"/([0-9]{4})/([0-9]{2})/([0-9]{2})/\.$", jscache_day_dir)
        if not m:
            print "WARNING: junk in jscache dir at: " + jscache_day_dir
            continue
        year, month, day = m.groups()
        if not os.path.exists(os.path.join(data_root_dir, year, month, day)):
            days_to_update |= {(year, month, day)}

    if days_to_update:
        # Rebuild jscache where needed (including rebuilding top-level js data).
        update_jscache(days_to_update)
    else:
        # Even if no actual js data changed, regenerate top level because maybe
        # some color changed in config.pyon, or an annotation was added etc.
        rebuild_toplevel_jsdata()

def get_series_name(metric_name, target):
    concatenated = metric_name + ("_" + target if target else "")
    safe_chars = string.ascii_letters + string.digits + "_"
    safe_concatenated = "".join([ch if ch in safe_chars else "_" for ch in concatenated])
    # Note: using at most 80 chars here means that the final filename will be
    #       less than 80 + 40 (for hash) + 10 (for hostname) which is less than
    #       130 which safely below the max filename length on eCryptFS and other
    #       file system types that might have similar absurd constraints.
    series_name = safe_concatenated[0:80] + "_" + hashlib.sha1(concatenated).hexdigest()
    series_name = re.sub("__+", "_", series_name)
    # NOTE: series_name will be used as a Javascript identifier, and we
    # also build data filenames using: series_name + "-" + hostname + ".csv"
    # and assume everything left of the first "-" is the series_name
    assert SERIES_HOSTNAME_FILENAME_SEPARATOR not in series_name
    return series_name

def load_config():
    # First load config.pyon as if it was Python dict (i.e. sort of like JSON
    # but trailing commas and #-prefixed comments are allowed)
    loaded_config = eval(open(os.path.join(SCRIPT_DIR, "..", "..", "config.pyon")).read())

    # Then add a few things for convenience, like a list of all unique pairs of
    # (metric_name, target) that appear in at least one chart, these are the
    # series that we will measure later on. Each such series is also given a
    # "name" built from characters that are safe to use as filenames.
    metric_to_list_of_targets_map = {}
    for chart in loaded_config["charts"]:
        for chart_series in chart["chart_series"]:
            metric_name = chart_series["metric"]
            target = chart_series.get("target", "")
            metric_to_list_of_targets_map.setdefault(metric_name, set()).add(target)

    unique_series = []
    for metric_name, target_list in metric_to_list_of_targets_map.iteritems():
        for target in target_list:
            unique_series.append({
                "metric_name": metric_name,
                "target": target,
                "name": get_series_name(metric_name, target)
            })

    loaded_config["unique_series"] = unique_series

    return loaded_config


def main():
    if args.subcommand == "measure":
        subcommand_measure()
    elif args.subcommand == "ls":
        subcommand_ls()
    elif args.subcommand == "jscache":
        subcommand_jscache()
    elif args.subcommand == "help":
        top_parser.print_help()

if __name__ == '__main__':
    try:
        config = load_config()
        data_root_dir = os.path.normpath(os.path.join(SCRIPT_DIR, "..", "..", "data"))
        jscache_root_dir = os.path.normpath(os.path.join(SCRIPT_DIR, "..", "..", "jscache"))
        jsdata_filename = os.path.normpath(os.path.join(SCRIPT_DIR, "..", "..", "allChartData.js"))

        top_parser = argparse.ArgumentParser()
        subparsers = top_parser.add_subparsers(help='subcommands', dest="subcommand")

        measure_parser = subparsers.add_parser("measure")
        measure_parser.add_argument("-q", "--quiet", action="store_true")
        measure_parser.add_argument("-v", "--verbose", action="store_true")
        measure_parser.add_argument("-s", "--force-sleep", metavar="SECONDS", type=float)
        measure_parser.add_argument("-m", "--metric", metavar="METRIC_NAME", action="append", help="Refresh measurements for all series using the specified metric (and all associated targets). This parameter can be passed several times.")
        measure_parser.add_argument("-c", "--chart", metavar="CHART_ID", action="append", help="Refresh measurements for the specified chart. This parameter can be passed several times.")

        jscache_parser = subparsers.add_parser("jscache")
        jscache_parser.add_argument("-q", "--quiet", action="store_true")

        ls_parser = subparsers.add_parser("ls")
        ls_parser.add_argument("-q", "--quiet", action="store_true")

        subparsers.add_parser("help")

        args = top_parser.parse_args()

        sys.exit(main())
    except KeyboardInterrupt:
        print
