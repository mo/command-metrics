#!/usr/bin/env python

# TODO:
# -- mark single metric as "bulk" (can run in parallel to others) or "perf" (must have machine to itself when it runs)
# -- re-run X times and save average
# -- make color attribute optional (auto generate unique colors from nice scale)
# -- minimum_remeasurement_interval per metric
# -- make little enum for exit codes

import os
import re
import sys
import csv
import stat
import time
import glob
import json
import errno
import shutil
import string
import hashlib
import platform
import tempfile
import argparse
import datetime
import subprocess

SCRIPT_DIR = os.path.dirname(os.path.realpath(__file__))
SERIES_HOSTNAME_FILENAME_SEPARATOR = "-"
FULL_DATETIME_FORMAT = "%Y-%m-%d %H:%M:%S"

def human_readable_time_span(seconds):
    if seconds < 0:
        return "-" + human_readable_time_span(-1*seconds)

    output = ""
    if seconds == 0 or seconds % 60 != 0:
        output = str(int(seconds % 60)) + "s"

    minutes = seconds // 60
    if minutes % 60 != 0:
        output = str(int(minutes % 60)) + "m" + output

    hours = minutes // 60
    if hours % 24 != 0:
        output = str(int(hours % 24)) + "h" + output

    days = hours // 24
    if days != 0:
        output = str(int(days)) + "d" + output

    return output

def parse_datetime(dt_str):
    if len(dt_str) == 10:
        timestamp = datetime.datetime.strptime(dt_str, "%Y-%m-%d")
    elif len(dt_str) == 16:
        timestamp = datetime.datetime.strptime(dt_str, "%Y-%m-%d %H:%M")
    else:
        timestamp = datetime.datetime.strptime(dt_str, "%Y-%m-%d %H:%M:%S")
    return timestamp


def add_to_path_if_exists(target_env, folder):
    if os.path.exists(folder):
        if args.verbose:
            print "[DEBUG] add '%s' to PATH" % folder
        target_env["PATH"] = target_env["PATH"] + ":" + folder

def prepend_to_path_if_exists(target_env, folder):
    if os.path.exists(folder):
        if args.verbose:
            print "[DEBUG] preprend '%s' to PATH" % folder
        target_env["PATH"] = folder + ":" + target_env["PATH"]

def mkdir_p(path):
    try:
        os.makedirs(path)
    except OSError as exc:
        if exc.errno == errno.EEXIST and os.path.isdir(path):
            pass
        else:
            raise

def print_msg_with_details(msg, exitcode, metric_command, metric_name, target, stdout, stderr):
    fullmsg = (msg + "\n"
               + "          metric: " + metric_name + "\n"
               + "          target: " + target + "\n"
               + "          metric command: " + metric_command + "\n"
               + "          exitcode: " + str(exitcode) + "\n"
               + "          stdout: " + repr(stdout) + "\n"
               + "          stderr: " + repr(stderr) + "\n")
    print fullmsg

def measure_metric(metric_name, target):
    metric = config["metrics"][metric_name]

    if "cmd_sleep_seconds" in metric:
        if args.force_sleep != None:
            cmd_sleep_seconds = args.force_sleep
        else:
            cmd_sleep_seconds = float(metric["cmd_sleep_seconds"])
        if args.verbose:
            if ("%.3f" % cmd_sleep_seconds).endswith(".000"):
                sleep_time_str = "%.0f" % cmd_sleep_seconds
            else:
                sleep_time_str = "%.3f" % cmd_sleep_seconds
            print "[DEBUG] throttle sleeping for %s seconds" % sleep_time_str
        time.sleep(cmd_sleep_seconds)

    child_env = os.environ.copy()
    # Prepend (rather than append) the metric-scripts to $PATH because we want
    # measurements to have as little dependency on the host machine as possible.
    prepend_to_path_if_exists(child_env, os.path.join(SCRIPT_DIR, "metric-scripts"))
    external_scripts_root = os.path.join(SCRIPT_DIR, "../../external-metric-scripts")
    # If your metrics require scripts for different git repositories you can add
    # each such git repo as a submodule under the "external-metric-scripts" dir
    # and every dir in every such repo will be put on the $PATH during
    # measurement. You can also add metric scripts directly in
    # "external-metric-scripts" without using a git submodule if you want.
    if os.path.isdir(external_scripts_root):
        for dirpath, _, _ in os.walk(external_scripts_root, followlinks=True):
            prepend_to_path_if_exists(child_env, dirpath)
    # Don't prepend this because it has host specific dependencies, it's added
    # only as a convenience.
    add_to_path_if_exists(child_env, "/usr/local/bin")

    command_template = metric["cmd"]
    if args.verbose:
        print "[DEBUG] command template: " + command_template
    try:
        metric_command = command_template % {"target": target}
    except (TypeError, ValueError):
        # TypeError is raised if template string contains %f instead of %%f
        # ValueError is raised if template string contains %!
        print r"[ERROR] Invalid command template for metric '%s'." % metric_name
        print r"[NOTE] % must be escaped as %% and target should be referred to as %(target)s"
        print r"[NOTE] Offending command template was: " + command_template
        sys.exit(1)
    if args.verbose:
        print "[DEBUG] about to run command: " + metric_command

    proc = subprocess.Popen(metric_command, env=child_env, shell=True, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    stdout_output, stderr_output = proc.communicate()
    stdout_output = stdout_output.strip()
    stderr_output = stderr_output.strip() if stderr_output else ''
    if proc.returncode != 0:
        print_msg_with_details("[WARNING] metric command returned non-zero exit code", proc.returncode, metric_command, metric_name, target, stdout_output, stderr_output)
    if args.verbose:
        print "[DEBUG] value (from stdout) was: " + repr(stdout_output)
    try:
        value = float(stdout_output)
    except ValueError:
        # The metric script printed something other than a number to stdout.
        # Possibly an error message or similar, at any rate we have failed to
        # make this particular measurement, suppress error so we can move on to
        # the next scheduled measurement instead.
        print_msg_with_details("[ERROR] value printed to stdout by metric command was not a number", proc.returncode, metric_command, metric_name, target, stdout_output, stderr_output)
        value = None

    require_value_above = metric.get("require", {}).get("value", {}).get("above", None)
    require_value_below = metric.get("require", {}).get("value", {}).get("below", None)

    if (require_value_above != None and value <= require_value_above) or \
       (require_value_below != None and value >= require_value_below):
        if args.verbose:
            print_msg_with_details("[DEBUG] value %s is outside of above/below range "
                                   "limits in metric config, will not save to disk." % repr(value),
                                   proc.returncode, metric_command, metric_name, target,
                                   stdout_output, stderr_output)
        value = None

    if args.verbose:
        print ""
    return value


class MeasuredData(object):
    CSV_DELIMITER_CHAR = ','
    CSV_QUOTE_CHAR = '"'

    def __init__(self):
        # self.data is a map from series_name to a list of (timestamp_str, value)
        # NOTE: series_name is computed using get_series_name(metric_name, target)
        self.data = {}

    @staticmethod
    def append_data_to_file(metric_name, target, timestamp, value):
        (year, month, day) = get_year_month_day(timestamp)
        target_data_dir = os.path.join(data_series_root_dir, year, month, day)
        series_name = get_series_name(metric_name, target)
        csv_filepath = os.path.join(target_data_dir, series_name + SERIES_HOSTNAME_FILENAME_SEPARATOR + platform.node() + ".csv")

        timestamp_str = timestamp.strftime(FULL_DATETIME_FORMAT)
        mkdir_p(os.path.dirname(csv_filepath))
        with open(csv_filepath, "ab") as data_file:
            writer = csv.writer(data_file, delimiter=MeasuredData.CSV_DELIMITER_CHAR, quotechar=MeasuredData.CSV_QUOTE_CHAR, quoting=csv.QUOTE_MINIMAL)
            writer.writerow([timestamp_str, value])

    def add_data_from_file(self, datafile):
        basename = os.path.basename(datafile)
        series_name, _ = basename.split(SERIES_HOSTNAME_FILENAME_SEPARATOR, 1)
        with open(datafile, 'rb') as csvfile:
            csv_reader = csv.reader(csvfile, delimiter=MeasuredData.CSV_DELIMITER_CHAR, quotechar=MeasuredData.CSV_QUOTE_CHAR)
            for row in csv_reader:
                try:
                    [timestamp_str, value] = row
                    self.data.setdefault(series_name, []).append((timestamp_str, value))
                except ValueError as e:
                    print "ERROR: invalid line in incoming data file: " + datafile
                    print "Line parts: " + repr(row)
                    print "Error details: " + repr(e)
                    sys.exit(1)

    def sort(self):
        for _, series_data in self.data.iteritems():
            series_data.sort()

    @staticmethod
    def load_from_dir(data_dir):
        measured_data = MeasuredData()
        for fn in os.listdir(data_dir):
            csv_filepath = os.path.join(data_dir, fn)
            measured_data.add_data_from_file(csv_filepath)
        measured_data.sort()
        return measured_data

    @staticmethod
    def load_from_file(csv_filepath):
        measured_data = MeasuredData()
        measured_data.add_data_from_file(csv_filepath)
        return measured_data

    def save_to_file(self, csv_filepath):
        mkdir_p(os.path.dirname(csv_filepath))
        with open(csv_filepath, "wb") as data_file:
            writer = csv.writer(data_file, delimiter=MeasuredData.CSV_DELIMITER_CHAR, quotechar=MeasuredData.CSV_QUOTE_CHAR, quoting=csv.QUOTE_MINIMAL)
            for series_name, series_data in self.data.iteritems():
                for [timestamp_str, value] in series_data:
                    writer.writerow([timestamp_str, value])

    def get_series_data(self, series_name):
        return self.data.get(series_name, [])

    def set_series_data(self, series_name, series_data):
        self.data[series_name] = series_data

    # Returns True is something was deleted.
    def delete_series(self, series_name):
        return self.data.pop(series_name, None) != None


def measure_series(selected_series):
    day_dirs_written_to = set()
    total = len(selected_series)
    if args.verbose:
        print "[DEBUG] prepared todo list of %d things to measure" % total

    for (idx, (metric_name, target)) in enumerate(selected_series):
        if not args.quiet:
            print_dict = {
                "current": idx+1,
                "total": total,
                "metric_name": metric_name,
                "target": target,
            }
            if target:
                print "[%(current)d/%(total)d] measuring '%(metric_name)s' for '%(target)s'" % print_dict
            else:
                print "[%(current)d/%(total)d] measuring '%(metric_name)s'" % print_dict
        value = measure_metric(metric_name, target)

        # value == None means that the measurement failed (e.g. a metric script
        # printed non-number to stdout), start the next measurement instead.
        if value != None:
            now = datetime.datetime.now()
            MeasuredData.append_data_to_file(metric_name, target, now, value)
            (year, month, day) = get_year_month_day(now)
            day_dirs_written_to |= {(year, month, day)}

    return day_dirs_written_to

def get_year_month_day(some_datetime):
    return (some_datetime.strftime("%Y"), some_datetime.strftime("%m"), some_datetime.strftime("%d"))

def find_csv_days():
    csv_days = []
    for day_dir in glob.glob(os.path.join(data_series_root_dir, "*/*/*")):
        m = re.search("/([0-9]{4})/([0-9]{2})/([0-9]{2})$", day_dir)
        if not m:
            print "WARNING: junk in data dir at: " + day_dir
            continue
        year, month, day = m.groups()
        csv_days.append([year, month, day, day_dir])
    return csv_days

def subcommand_delete():
    if not args.chart and not args.metric and not args.target:
        print "ERROR: You must specify at least one argument that specifies what to delete, i.e. --chart, --metric, --target or a combination of those."
        print "       Run subcommand delete with --help for more information."
        sys.exit(1)

    if args.metric and not any(serie["metric_name"] == args.metric for serie in config["unique_series"]):
        print "ERROR: invalid metric name specified in --metric/-m parameter: " + args.metric
        sys.exit(1)

    if args.target and not any(serie["target"] == args.target for serie in config["unique_series"]):
        print "ERROR: invalid target specified in --target/-t parameter: " + args.target
        sys.exit(1)

    if args.chart and not any(chart["chart_id"] == args.chart for chart in config["charts"]):
        print "ERROR: invalid chart_id specified in --chart/-c parameter: " + args.chart
        sys.exit(1)

    selected_series = set()
    for chart in config["charts"]:
        if args.chart and chart["chart_id"] != args.chart:
            continue

        for chart_series_declaration in chart["chart_series"]:

            for resolved_serie in get_resolved_series_for_chart_series_entry(chart_series_declaration):
                metric_name = resolved_serie["metric"]
                target = resolved_serie["target"]

                if args.metric and metric_name != args.metric:
                    continue

                if args.target and target != args.target:
                    continue

                selected_series.add(get_series_name(metric_name, target))

    days_to_update = set()
    for [year, month, day, day_dir] in find_csv_days():
        for csv_file in os.listdir(day_dir):
            if not csv_file.endswith(".csv"):
                print "WARNING: junk file in data dir at: " + os.path.join(day_dir, csv_file)
                continue
            csv_file_series_name, _ = csv_file.split(SERIES_HOSTNAME_FILENAME_SEPARATOR, 1)
            if csv_file_series_name in selected_series:
                csv_filepath = os.path.join(day_dir, csv_file)
                loaded_csv = MeasuredData.load_from_file(csv_filepath)
                original_series_data = loaded_csv.get_series_data(csv_file_series_name)

                def should_delete_datapoint(timestamp_str, value):
                    timestamp = datetime.datetime.strptime(timestamp_str, FULL_DATETIME_FORMAT)
                    if args.date_after != None and timestamp <= args.date_after:
                        return False
                    if args.date_before != None and timestamp >= args.date_before:
                        return False

                    if args.value != None and value != args.value:
                        return False
                    if args.value_above != None and value <= args.value_above:
                        return False
                    if args.value_below != None and value >= args.value_below:
                        return False

                    return True

                updated_series_data = [(timestamp_str, value) for [timestamp_str, value] in original_series_data if not should_delete_datapoint(timestamp_str, float(value))]
                if len(original_series_data) != len(updated_series_data):
                    loaded_csv.set_series_data(csv_file_series_name, updated_series_data)
                    loaded_csv.save_to_file(csv_filepath)
                    days_to_update |= {(year, month, day)}

    update_jscache(days_to_update)


def get_targetlist_filepath(target_list_id):
    return os.path.join(data_targetlists_root_dir, get_safe_name(target_list_id) + ".lst")


def load_existing_entries_for_dynamic_target_list(target_list_id):
    target_list_filepath = get_targetlist_filepath(target_list_id)
    if os.path.exists(target_list_filepath):
        existing_target_ids = [t for t in open(target_list_filepath).read().splitlines() if t.strip() != ""]
    else:
        existing_target_ids = []
    return existing_target_ids


def get_resolved_series_for_chart_series_entry(chart_series_entry):
    resolved_series = []

    target_list_id = chart_series_entry.get("target_list", "")
    if target_list_id:
        target_list = config["target_lists"][target_list_id]
        if target_list["type"] == "static":
            for target_list_entry in target_list["target_entries"]:
                target = target_list_entry["target"]
                resolved_series.append({
                    "metric": chart_series_entry["metric"],
                    "target": target,
                    "color": target_list_entry.get("color", chart_series_entry.get("color", None)),
                })
        elif target_list["type"] == "dynamic":
            target_ids = load_existing_entries_for_dynamic_target_list(target_list_id)
            for target in target_ids:
                resolved_series.append({
                    "metric": chart_series_entry["metric"],
                    "target": target,
                    "color": chart_series_entry.get("color", None),
                })

    target = chart_series_entry.get("target", "")
    if target:
        resolved_series.append({
            "metric": chart_series_entry["metric"],
            "target": target,
            "color": chart_series_entry.get("color", None),
        })


    return resolved_series


def refresh_dynamic_target_list(target_list_id, refresh_cmd):
    if args.verbose:
        print("[DEBUG] will refresh dynamic target list '" + target_list_id +  "' by running command '" + refresh_cmd + "'")

    proc = subprocess.Popen(refresh_cmd, shell=True, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    stdout_output, stderr_output = proc.communicate()
    stdout_output = stdout_output.strip()
    stderr_output = stderr_output.strip() if stderr_output else ''
    if proc.returncode != 0:
        print("[WARNING] dynamic target list command returned non-zero exit code")
        print("                exit code: " + proc.returncode)
        print("                refresh_cmd: " + refresh_cmd)
        print("                stdout_output: " + stdout_output)
        print("                stderr_output: " + stderr_output)
    if args.verbose:
        print "[DEBUG] value (from stdout) was: " + repr(stdout_output)

    new_target_ids = [t for t in stdout_output.splitlines() if t.strip() != ""]
    existing_target_ids = load_existing_entries_for_dynamic_target_list(target_list_id)

    unique_target_ids = list(set(existing_target_ids) | set(new_target_ids))

    mkdir_p(data_targetlists_root_dir)
    target_list_filepath = get_targetlist_filepath(target_list_id)
    with open(target_list_filepath, "w") as target_list_file:
        target_list_file.write("\n".join(unique_target_ids))


def subcommand_measure():
    if args.metric:
        for metric_name in args.metric:
            if not any(serie["metric_name"] == metric_name for serie in config["unique_series"]):
                print "ERROR: invalid metric name specified in --metric/-m parameter: " + metric_name
                sys.exit(1)

    if args.chart:
        for chart_id in args.chart:
            if not any(chart["chart_id"] == chart_id for chart in config["charts"]):
                print "ERROR: invalid chart_id specified in --chart/-c parameter: " + chart_id
                sys.exit(1)

    # Use unbuffered stdout
    sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 0)

    # If "--metric foo --metric yada" is passed, then only re-take measurements
    # for those metrics. If --chart foo --chart bar is passed, then re-take
    # measurements for all series (metric/target pairs) that appears in those
    # charts. If --metric and --chart parameters are passed simultaneously then
    # re-take measurements only for series in the passed chart, that is also
    # using the specified metric.
    selected_series = set()
    for chart in config["charts"]:
        # Typically one "series_entry" refers to a single series, but if the
        # series_entry has a target_list attribute, it results in a set of
        # series instead.
        for chart_series_declaration in chart["chart_series"]:

            metric_name = chart_series_declaration["metric"]

            if not chart_series_declaration.get("enabled", True):
                continue

            if args.metric and not metric_name in args.metric:
                continue

            if args.chart and not chart["chart_id"] in args.chart:
                continue

            target_list_id = chart_series_declaration.get("target_list", None)
            if target_list_id:
                target_list = config["target_lists"][target_list_id]
                if target_list["type"] == "dynamic":
                    refresh_dynamic_target_list(target_list_id, target_list["cmd"])

            for resolved_serie in get_resolved_series_for_chart_series_entry(chart_series_declaration):
                selected_series.add((resolved_serie["metric"], resolved_serie["target"]))

    measurement_start_time = datetime.datetime.now()
    if not args.quiet:
        print "Measurement started: " + str(measurement_start_time)[:19]

    day_dirs_written_to = measure_series(selected_series)

    measurement_stoptime = datetime.datetime.now()
    measurement_seconds = (measurement_stoptime - measurement_start_time).total_seconds()
    if not args.quiet:
        print "Measurement stopped: " + str(measurement_stoptime)[:19]
        print "Measurement took: " + human_readable_time_span(measurement_seconds)

    # day_dirs_written_to will typically be an array of a single day, except for
    # example if measure_series() started at 23:50 and took 20 mins to run.
    update_jscache(day_dirs_written_to)

def subcommand_import():
    if not any(serie["metric_name"] == args.metric_name for serie in config["unique_series"]):
        print "ERROR: invalid metric name specified: " + args.metric_name
        sys.exit(1)

    if args.target:
        if not any(serie["target"] == args.target for serie in config["unique_series"]):
            print "ERROR: invalid target specified in --target/-t parameter: " + args.target
            sys.exit(1)

    if not os.path.exists(args.csv_file):
        print "ERROR: file not found '%s'" % args.csv_file
        sys.exit(1)

    day_dirs_written_to = set()

    with open(args.csv_file, 'rb') as csvfile:
        csv_reader = csv.reader(csvfile, delimiter=MeasuredData.CSV_DELIMITER_CHAR, quotechar=MeasuredData.CSV_QUOTE_CHAR)
        for row in csv_reader:
            try:
                [timestamp_str, value] = row

                timestamp = datetime.datetime.strptime(timestamp_str, FULL_DATETIME_FORMAT)
                MeasuredData.append_data_to_file(args.metric_name, args.target, timestamp, value)
                (year, month, day) = get_year_month_day(timestamp)
                day_dirs_written_to |= {(year, month, day)}

            except ValueError as e:
                print "ERROR: invalid line in incoming data file: " + args.csv_file
                print "Details: " + repr(e)
                sys.exit(1)

    update_jscache(day_dirs_written_to)

def subcommand_ls():
    series_name_len = max([len(serie["name"]) for serie in config["unique_series"]])
    metric_name_len = max([len(serie["metric_name"]) for serie in config["unique_series"]])
    target_len = max([len(serie.get("target", "")) for serie in config["unique_series"]])

    if not args.quiet:
        print "%s  %s  %s" % ("METRIC".ljust(metric_name_len),
                              "TARGET".ljust(target_len),
                              "SERIES".ljust(series_name_len))
    for serie in config["unique_series"]:
        series_name = serie["name"]
        metric_name = serie["metric_name"]
        target = serie.get("target", "")
        print "%s  %s  %s" % (metric_name.ljust(metric_name_len),
                              target.ljust(target_len),
                              series_name.ljust(series_name_len))

def dir_last_modified(target_dir):
    files = glob.glob(os.path.join(target_dir, "*"))
    return max([os.path.getmtime(f) for f in files])

def rebuild_jscache_day(year, month, day):
    day_jscache_dir = os.path.join(jscache_root_dir, year, month, day)
    if os.path.exists(day_jscache_dir):
        shutil.rmtree(day_jscache_dir)

    day_data_dir = os.path.join(data_series_root_dir, year, month, day)
    if not os.path.exists(day_data_dir):
        if not args.quiet:
            print "Deleted stale jscache for %s-%s-%s" % (year, month, day)
        # All data for this day has been deleted, deleting the corresponding
        # jscache dir (which we just did above) is the only thing we need to do.
        return

    if not args.quiet:
        print "Rebuilding jscache for %s-%s-%s" % (year, month, day)

    measured_data = MeasuredData.load_from_dir(day_data_dir)
    mkdir_p(day_jscache_dir)

    for serie in config["unique_series"]:
        series_name = serie["name"]
        series_data = measured_data.get_series_data(series_name)
        if series_data:
            serie_jscache_file = os.path.join(day_jscache_dir, series_name)
            jsvals = []
            for timestamp_str, value in series_data:
                timestamp = datetime.datetime.strptime(timestamp_str, FULL_DATETIME_FORMAT)
                timestamp_unix = timestamp.strftime("%s")
                jsvals.append("{x:%s,y:%s}," % (timestamp_unix, value))
            all_jsvals = "".join(jsvals)
            with open(serie_jscache_file + ".jsv", "w") as fil:
                fil.write(all_jsvals)

def reconcatenate_jscache_dir_from_subdirs(jscache_dir):
    for existing_jsv in glob.glob(jscache_dir + "/*.jsv"):
        os.remove(existing_jsv)
    for serie in config["unique_series"]:
        series_name = serie["name"]
        jscache_concatenated_jsv = os.path.join(jscache_dir, series_name + ".jsv")
        subdir_jsv_files = sorted(glob.glob(jscache_dir + "/*/" + series_name + ".jsv"))
        with open(jscache_concatenated_jsv, "wb") as concatenated:
            for subdir_jsv in subdir_jsv_files:
                shutil.copyfileobj(open(subdir_jsv, 'rb'), concatenated)

def rebuild_toplevel_jsdata():
    jsdata_tempfile_oshandle, jsdata_tempfile_name = tempfile.mkstemp()
    with open(jsdata_tempfile_name, "w") as jsdata_temp_file:
        for serie in config["unique_series"]:
            series_name = serie["name"]
            series_jsv = os.path.join(jscache_root_dir, series_name + ".jsv")
            if os.path.exists(series_jsv):
                # series_jsv wont exist if it was added to config.json but no measurement has been done yet
                jsdata_temp_file.write("const %s = [\n" % series_name)
                shutil.copyfileobj(open(series_jsv, 'rb'), jsdata_temp_file)
                jsdata_temp_file.write("\n];\n\n")

        jsdata_temp_file.write("allChartData = [\n")
        for chart in config["charts"]:
            jsdata_temp_file.write("  {\n")
            jsdata_temp_file.write("    'chartKey': '%s',\n" % chart["chart_id"])
            jsdata_temp_file.write("    'chartTitle': %s,\n" % json.dumps(chart["chart_title"]))
            jsdata_temp_file.write("    'chartSeries': [\n")

            for chart_series_declaration in chart["chart_series"]:
                metric_name = chart_series_declaration["metric"]
                metric = config["metrics"][metric_name]
                for resolved_serie in get_resolved_series_for_chart_series_entry(chart_series_declaration):
                    target = resolved_serie.get("target", None)
                    series_title = resolved_serie.get("title", None)
                    if not series_title:
                        series_title = metric.get("title", metric["cmd"]) % {"target": target}
                    jsdata_temp_file.write("            {\n")
                    # using json.dumps() to escape string in case it contains ' and " etc
                    jsdata_temp_file.write("              'name': %s,\n" % json.dumps(series_title))
                    if resolved_serie.get("color", None):
                        jsdata_temp_file.write("              'color': '%s',\n" % resolved_serie["color"])
                    jsdata_temp_file.write("              'data': %s\n" % get_series_name(metric_name, target))
                    jsdata_temp_file.write("            },\n")
            jsdata_temp_file.write("    ],\n")

            if "annotations" in chart:
                jsdata_temp_file.write("    chartAnnotations: {\n")
                for annotation_item in chart["annotations"]:
                    if type(annotation_item) is str:
                        annotations = config["annotation_sets"][annotation_item]
                    elif type(annotation_item) is dict:
                        annotations = annotation_item

                    for date, description in annotations.iteritems():
                        timestamp = parse_datetime(date).strftime("%s")
                        jsdata_temp_file.write("            '%s': %s,\n" % (timestamp, json.dumps(description)))

                jsdata_temp_file.write("    }\n")

            jsdata_temp_file.write("  },\n")
        jsdata_temp_file.write("]\n")

    os.close(jsdata_tempfile_oshandle)
    shutil.move(jsdata_tempfile_name, jsdata_filename)
    os.chmod(jsdata_filename, stat.S_IRUSR | stat.S_IWUSR | stat.S_IROTH)

def update_jscache(days_to_update):
    for (year, month, day) in days_to_update:
        rebuild_jscache_day(year, month, day)

    months_to_update = set([(year, month) for (year, month, day) in days_to_update])
    for (year, month) in months_to_update:
        jsv_month_dir = os.path.join(jscache_root_dir, year, month)
        reconcatenate_jscache_dir_from_subdirs(jsv_month_dir)

    years_to_update = set([year for (year, month, day) in days_to_update])
    for year in years_to_update:
        jsv_year_dir = os.path.join(jscache_root_dir, year)
        reconcatenate_jscache_dir_from_subdirs(jsv_year_dir)

    reconcatenate_jscache_dir_from_subdirs(jscache_root_dir)

    rebuild_toplevel_jsdata()

def subcommand_jscache():
    # Figure out which days that needs to have their jscache entries rebuilt.
    days_to_update = set()
    for [year, month, day, day_dir] in find_csv_days():
        jscache_day_dir = os.path.join(jscache_root_dir, year, month, day)
        if not os.path.isdir(jscache_day_dir) or os.listdir(jscache_day_dir) == []:
            days_to_update |= {(year, month, day)}
        else:
            data_last_modified = dir_last_modified(day_dir)
            jscache_last_modified = dir_last_modified(jscache_day_dir)
            if data_last_modified > jscache_last_modified:
                days_to_update |= {(year, month, day)}

    # Find days we have jscache data for but where the csv data is deleted.
    for jscache_day_dir in glob.glob(os.path.join(jscache_root_dir, "*/*/*/.")):
        m = re.search(r"/([0-9]{4})/([0-9]{2})/([0-9]{2})/\.$", jscache_day_dir)
        if not m:
            print "WARNING: junk in jscache dir at: " + jscache_day_dir
            continue
        year, month, day = m.groups()
        if not os.path.exists(os.path.join(data_series_root_dir, year, month, day)):
            days_to_update |= {(year, month, day)}

    if days_to_update:
        # Rebuild jscache where needed (including rebuilding top-level js data).
        update_jscache(days_to_update)
    else:
        # Even if no actual js data changed, regenerate top level because maybe
        # some color changed in config.pyon, or an annotation was added etc.
        rebuild_toplevel_jsdata()


def get_series_name(metric_name, target):
    return get_safe_name(metric_name + ("_" + target if target else ""))


# Keeps only characters that are safe for use in filenames, and makes sure the
# safe_name is not too long but still ensures that the safe_name is unique.
def get_safe_name(full_name):
    safe_chars = string.ascii_letters + string.digits + "_"
    name_safe_chars = "".join([ch if ch in safe_chars else "_" for ch in full_name])
    # Note: using at most 80 chars here means that the final filename will be
    #       less than 80 + 40 (for hash) + 10 (for hostname) which is less than
    #       130 which safely below the max filename length on eCryptFS and other
    #       file system types that might have similar absurd constraints.
    safe_name = name_safe_chars[0:80] + "_" + hashlib.sha1(full_name).hexdigest()
    safe_name = re.sub("__+", "_", safe_name)
    # NOTE: safe_name will be used as a Javascript identifier, and we
    # also build data filenames using: safe_name + "-" + hostname + ".csv"
    # and assume everything left of the first "-" is the safe_name
    assert SERIES_HOSTNAME_FILENAME_SEPARATOR not in safe_name
    return safe_name


def load_raw_config():
    # First load config.pyon as if it was Python dict (i.e. sort of like JSON
    # but trailing commas and #-prefixed comments are allowed)
    raw_config = eval(open(os.path.join(SCRIPT_DIR, "..", "..", "config.pyon")).read())

    return raw_config

def resolve_config():
    # Add a few things for convenience, like a list of all unique pairs of
    # (metric_name, target) that appear in at least one chart, these are the
    # series that we will list in the "ls" command and it's also the set of series
    # that we will use to determine whether a --metric or --target param passed to
    # "delete" is valid. Each such series is also given a "name" built from
    # characters that are safe to use as filenames.
    metric_to_list_of_targets_map = {}
    for chart in config["charts"]:
        for chart_series_declaration in chart["chart_series"]:
            metric_name = chart_series_declaration["metric"]
            for resolved_serie in get_resolved_series_for_chart_series_entry(chart_series_declaration):
                metric_to_list_of_targets_map.setdefault(resolved_serie["metric"], set()).add(resolved_serie["target"])

    unique_series = []
    for metric_name, target_list in metric_to_list_of_targets_map.iteritems():
        for target in target_list:
            unique_series.append({
                "metric_name": metric_name,
                "target": target,
                "name": get_series_name(metric_name, target)
            })

    config["unique_series"] = unique_series


def main():
    if args.subcommand == "measure":
        subcommand_measure()
    elif args.subcommand == "import":
        subcommand_import()
    elif args.subcommand == "ls":
        subcommand_ls()
    elif args.subcommand == "jscache":
        subcommand_jscache()
    elif args.subcommand == "delete":
        subcommand_delete()
    elif args.subcommand == "help":
        top_parser.print_help()

if __name__ == '__main__':
    try:
        data_series_root_dir = os.path.normpath(os.path.join(SCRIPT_DIR, "..", "..", "data", "series"))
        data_targetlists_root_dir = os.path.normpath(os.path.join(SCRIPT_DIR, "..", "..", "data", "targetlists"))
        jscache_root_dir = os.path.normpath(os.path.join(SCRIPT_DIR, "..", "..", "jscache"))
        jsdata_filename = os.path.normpath(os.path.join(SCRIPT_DIR, "..", "..", "allChartData.js"))

        config = load_raw_config()
        resolve_config()

        top_parser = argparse.ArgumentParser()
        subparsers = top_parser.add_subparsers(help='subcommands', dest="subcommand")

        measure_parser = subparsers.add_parser("measure")
        measure_parser.add_argument("-q", "--quiet", action="store_true")
        measure_parser.add_argument("-v", "--verbose", action="store_true")
        measure_parser.add_argument("-s", "--force-sleep", metavar="SECONDS", type=float)
        measure_parser.add_argument("-m", "--metric", metavar="METRIC_NAME", action="append", help="Refresh measurements for all series using the specified metric (and all associated targets). This parameter can be passed several times.")
        measure_parser.add_argument("-c", "--chart", metavar="CHART_ID", action="append", help="Refresh measurements for the specified chart. This parameter can be passed several times.")

        measure_parser = subparsers.add_parser("import")
        measure_parser.add_argument("metric_name", metavar="METRIC_NAME", type=str, help="Metric name under which data should be stored (must already exist).")
        measure_parser.add_argument("csv_file", metavar="CSV_FILE", type=str, help="CSV file containing data to be imported (lines must be in 'YYYY-MM-DD HH:MM:SS,value' format).")
        measure_parser.add_argument("-q", "--quiet", action="store_true")
        measure_parser.add_argument("-v", "--verbose", action="store_true")
        measure_parser.add_argument("-t", "--target", metavar="TARGET", type=str, default="", help="Optional target name used when storing the data (must already exist).")

        jscache_parser = subparsers.add_parser("jscache")
        jscache_parser.add_argument("-q", "--quiet", action="store_true")

        ls_parser = subparsers.add_parser("ls")
        ls_parser.add_argument("-q", "--quiet", action="store_true")

        delete_parser = subparsers.add_parser("delete")
        delete_parser.add_argument("-q", "--quiet", action="store_true")
        delete_parser.add_argument("-m", "--metric", metavar="METRIC_NAME", type=str, help="Delete datapoints in series using the specified metric.")
        delete_parser.add_argument("-t", "--target", metavar="TARGET", type=str, default="", help="Delete datapoints in series with the specified TARGET.")
        delete_parser.add_argument("-c", "--chart", metavar="CHART_ID", type=str, help="Delete datapoints in series that appear in the chart with CHART_ID (NOTE: this also removes datapoints from other charts if the same series appears in multiple charts).")
        delete_parser.add_argument("--value-above", metavar="VALUE", type=float, help="Only delete datapoints where the value is strictly above VALUE.")
        delete_parser.add_argument("--value-below", metavar="VALUE", type=float, help="Only delete datapoints where the value is strictly below VALUE.")
        delete_parser.add_argument("--value", metavar="VALUE", type=float, help="Delete datapoints where the value is VALUE.")
        delete_parser.add_argument("--date-before", metavar="DATE", type=parse_datetime, help="Only delete datapoints where the date is strictly before DATE.")
        delete_parser.add_argument("--date-after", metavar="DATE", type=parse_datetime, help="Only delete datapoints where the date is strictly after DATE.")

        subparsers.add_parser("help")

        args = top_parser.parse_args()

        sys.exit(main())
    except KeyboardInterrupt:
        print
